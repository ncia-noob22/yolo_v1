{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Necessary Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data and Directory Configurations\n",
    "import os.path as osp\n",
    "\n",
    "DATA_DIR = '/data/torch/VOCdetection'\n",
    "ROOT_DIR = '~/workspace/yolo_v1/kk/' # Customize!!!\n",
    "\n",
    "# Model Configurations\n",
    "S = 7\n",
    "B = 2\n",
    "C = 20\n",
    "INPUT_W, INPUT_H = 448, 448\n",
    "\n",
    "# Dataset Configurations\n",
    "CLASSES = {\n",
    "    'person' : 0,\n",
    "    'bird' : 1,\n",
    "    'cat' : 2,\n",
    "    'cow' : 3,\n",
    "    'dog' : 4,\n",
    "    'horse' : 5,\n",
    "    'sheep' : 6,\n",
    "    'aeroplane' : 7,\n",
    "    'bicycle' : 8,\n",
    "    'boat' : 9,\n",
    "    'bus' : 10,\n",
    "    'car' : 11,\n",
    "    'motorbike' : 12,\n",
    "    'train' : 13,\n",
    "    'bottle' : 14,\n",
    "    'chair' : 15,\n",
    "    'diningtable' : 16,\n",
    "    'pottedplant' : 17,\n",
    "    'sofa' : 18,\n",
    "    'tvmonitor' : 19,\n",
    "}\n",
    "\n",
    "# Learning Configurations\n",
    "BATCH_SIZE = 4\n",
    "MAX_EPOCHS = 1\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "        batch : list of (image, annotation)\n",
    "    Returns\n",
    "        (batch_x, batch_y)\n",
    "        batch_x : torch.Tensor with size [BATCH_SIZE, 3, 448, 448]\n",
    "        batch_y : torch.Tensor with size [BATCH_SIZE, S, S, 5 + C]\n",
    "    \"\"\"\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for image, annotation in batch:\n",
    "        # 0. Get Image Informations\n",
    "        image_w, image_h = image.size\n",
    "\n",
    "        # 1. convert image and append to 'xs'\n",
    "        x = transforms.PILToTensor()(image) # TODO : torch.transfroms -> Albumentations\n",
    "        x = transforms.ConvertImageDtype(torch.float32)(x)\n",
    "        x = transforms.Resize((INPUT_W, INPUT_H))(x)\n",
    "        xs.append(x)\n",
    "\n",
    "        # 2. parse annotation file and append to 'ys'\n",
    "        # 2.1. for each object in image\n",
    "        y = torch.zeros(S, S, 5 + C, dtype=torch.float32)\n",
    "        for obj in annotation['annotation']['object']:\n",
    "            obj_xmax = int(obj['bndbox']['xmax'])\n",
    "            obj_ymax = int(obj['bndbox']['ymax'])\n",
    "            obj_xmin = int(obj['bndbox']['xmin'])\n",
    "            obj_ymin = int(obj['bndbox']['ymin'])\n",
    "\n",
    "            # 2.2. normalize x, y, w, h\n",
    "            obj_x_center = (obj_xmax - obj_xmin) / 2\n",
    "            obj_y_center = (obj_ymax - obj_ymin) / 2\n",
    "\n",
    "            obj_x_float = obj_x_center / image_w * S # floating number between 0 and S\n",
    "            obj_y_float = obj_y_center / image_h * S # floating number between 0 and S\n",
    "\n",
    "            obj_x = obj_x_float - int(obj_x_float)\n",
    "            obj_y = obj_y_float - int(obj_y_float)\n",
    "\n",
    "            obj_w = (obj_xmax - obj_xmin) / image_w\n",
    "            obj_h = (obj_ymax - obj_ymin) / image_h\n",
    "\n",
    "            cell_col = int(obj_x_float)\n",
    "            cell_row = int(obj_y_float)\n",
    "            \n",
    "            y[cell_row, cell_col, 0] = obj_x\n",
    "            y[cell_row, cell_col, 1] = obj_y\n",
    "            y[cell_row, cell_col, 2] = obj_w\n",
    "            y[cell_row, cell_col, 3] = obj_h\n",
    "\n",
    "            # 2.3. confidence\n",
    "            y[cell_row, cell_col, 4] = 1 # must be multiplied with IoU later\n",
    "\n",
    "            # 2.4. class vectorize\n",
    "            class_idx = CLASSES[obj['name']]\n",
    "            y[cell_row, cell_col, B + class_idx] = 1\n",
    "\n",
    "        # 2.5. append y to 'ys'\n",
    "        ys.append(y)\n",
    "    \n",
    "    # 3. stack 'xs' and 'ys' and return\n",
    "    return torch.stack(xs, dim=0), torch.stack(ys, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /data/torch/VOCdetection/VOCtrainval_11-May-2012.tar\n",
      "Extracting /data/torch/VOCdetection/VOCtrainval_11-May-2012.tar to /data/torch/VOCdetection\n"
     ]
    }
   ],
   "source": [
    "dataset = torchvision.datasets.VOCDetection(\n",
    "    root=DATA_DIR,\n",
    "    year='2012',\n",
    "    image_set='val',\n",
    "    download=True\n",
    ")\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=_collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build YOLO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLO(nn.Module):\n",
    "    def __init__(self, S=7, B=2, C=20, lambda_coord=5, lambda_noobj=0.5, device=DEVICE):\n",
    "        super(YOLO, self).__init__()\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_noobj = lambda_noobj\n",
    "        self.device = device\n",
    "\n",
    "        # Convolution Layers\n",
    "        # self.conv1 = nn.Sequential(\n",
    "        #     nn.Conv2d(3, 192, kernel_size=7, stride=2, padding=3),\n",
    "        #     nn.BatchNorm2d(192),\n",
    "        #     nn.ReLU()\n",
    "        # )\n",
    "        self.conv1 = nn.Conv2d(3, 192, kernel_size=7, stride=2, padding=3)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(192, 256, kernel_size=3, padding='same')\n",
    "\n",
    "        self.conv3 = nn.Conv2d(256, 128, kernel_size=1, padding='same')\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding='same')\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=1, padding='same')\n",
    "        self.conv6 = nn.Conv2d(256, 512, kernel_size=3, padding='same')\n",
    "\n",
    "        self.conv7 = nn.Conv2d(512, 256, kernel_size=1, padding='same')\n",
    "        self.conv8 = nn.Conv2d(256, 512, kernel_size=3, padding='same')\n",
    "        self.conv9 = nn.Conv2d(512, 256, kernel_size=1, padding='same')\n",
    "        self.conv10 = nn.Conv2d(256, 512, kernel_size=3, padding='same')\n",
    "        self.conv11 = nn.Conv2d(512, 256, kernel_size=1, padding='same')\n",
    "        self.conv12 = nn.Conv2d(256, 512, kernel_size=3, padding='same')\n",
    "        self.conv13 = nn.Conv2d(512, 256, kernel_size=1, padding='same')\n",
    "        self.conv14 = nn.Conv2d(256, 512, kernel_size=3, padding='same')\n",
    "        self.conv15 = nn.Conv2d(512, 512, kernel_size=1, padding='same')\n",
    "        self.conv16 = nn.Conv2d(512, 1024, kernel_size=3, padding='same')\n",
    "\n",
    "        self.conv17 = nn.Conv2d(1024, 512, kernel_size=1, padding='same')\n",
    "        self.conv18 = nn.Conv2d(512, 1024, kernel_size=3, padding='same')\n",
    "        self.conv19 = nn.Conv2d(1024, 512, kernel_size=1, padding='same')\n",
    "        self.conv20 = nn.Conv2d(512, 1024, kernel_size=3, padding='same')\n",
    "        self.conv21 = nn.Conv2d(1024, 1024, kernel_size=3, padding='same')\n",
    "        self.conv22 = nn.Conv2d(1024, 1024, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.conv23 = nn.Conv2d(1024, 1024, kernel_size=3, padding='same')\n",
    "        self.conv24 = nn.Conv2d(1024, 1024, kernel_size=3, padding='same')\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(7*7*1024, 4096)\n",
    "        self.fc2 = nn.Linear(4096, self.S * self.S * (5*self.B+self.C))\n",
    "\n",
    "        # Batch Normalization Layers\n",
    "        self.batch128 = nn.BatchNorm2d(128)\n",
    "        self.batch192 = nn.BatchNorm2d(192)\n",
    "        self.batch256 = nn.BatchNorm2d(256)\n",
    "        self.batch512 = nn.BatchNorm2d(512)\n",
    "        self.batch1024 = nn.BatchNorm2d(1024)\n",
    "\n",
    "        # other layers\n",
    "        self.maxPool = nn.MaxPool2d(2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch192(F.leaky_relu(self.conv1(x), negative_slope=0.1))\n",
    "        # x = self.conv1(x)\n",
    "        x = self.maxPool(x)\n",
    "\n",
    "        x = self.batch256(F.leaky_relu(self.conv2(x), negative_slope=0.1))\n",
    "        x = self.maxPool(x)\n",
    "\n",
    "        x = self.batch128(F.leaky_relu(self.conv3(x), negative_slope=0.1))\n",
    "        x = self.batch256(F.leaky_relu(self.conv4(x), negative_slope=0.1))\n",
    "        x = self.batch256(F.leaky_relu(self.conv5(x), negative_slope=0.1))\n",
    "        x = self.batch512(F.leaky_relu(self.conv6(x), negative_slope=0.1))\n",
    "        x = self.maxPool(x)\n",
    "\n",
    "        x = self.batch256(F.leaky_relu(self.conv7(x), negative_slope=0.1))\n",
    "        x = self.batch512(F.leaky_relu(self.conv8(x), negative_slope=0.1))\n",
    "        x = self.batch256(F.leaky_relu(self.conv9(x), negative_slope=0.1))\n",
    "        x = self.batch512(F.leaky_relu(self.conv10(x), negative_slope=0.1))\n",
    "        x = self.batch256(F.leaky_relu(self.conv11(x), negative_slope=0.1))\n",
    "        x = self.batch512(F.leaky_relu(self.conv12(x), negative_slope=0.1))\n",
    "        x = self.batch256(F.leaky_relu(self.conv13(x), negative_slope=0.1))\n",
    "        x = self.batch512(F.leaky_relu(self.conv14(x), negative_slope=0.1))\n",
    "        x = self.batch512(F.leaky_relu(self.conv15(x), negative_slope=0.1))\n",
    "        x = self.batch1024(F.leaky_relu(self.conv16(x), negative_slope=0.1))\n",
    "        x = self.maxPool(x)\n",
    "\n",
    "        x = self.batch512(F.leaky_relu(self.conv17(x), negative_slope=0.1))\n",
    "        x = self.batch1024(F.leaky_relu(self.conv18(x), negative_slope=0.1))\n",
    "        x = self.batch512(F.leaky_relu(self.conv19(x), negative_slope=0.1))\n",
    "        x = self.batch1024(F.leaky_relu(self.conv20(x), negative_slope=0.1))\n",
    "        x = self.batch1024(F.leaky_relu(self.conv21(x), negative_slope=0.1))\n",
    "        x = self.batch1024(F.leaky_relu(self.conv22(x), negative_slope=0.1))\n",
    "\n",
    "        x = self.batch1024(F.leaky_relu(self.conv23(x), negative_slope=0.1))\n",
    "        x = self.batch1024(F.leaky_relu(self.conv24(x), negative_slope=0.1))\n",
    "\n",
    "        x = nn.Flatten()(x)\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = x.view(BATCH_SIZE, self.S, self.S, -1)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def IOU(self, rect1, rect2):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            rect1, rect2 : torch.tensor with size (BATCH_SIZE, S, S, B)\n",
    "        Output\n",
    "            return : torch.tensor with size (BATCH_SIZE, S, S, B)\n",
    "        \"\"\"\n",
    "        rect1_x_max = rect1[..., 0:1] + rect1[..., 2:3] / (self.S * 2)\n",
    "        rect1_x_min = rect1[..., 0:1] - rect1[..., 2:3] / (self.S * 2)\n",
    "        rect1_y_max = rect1[..., 1:2] + rect1[..., 3:4] / (self.S * 2)\n",
    "        rect1_y_min = rect1[..., 1:2] - rect1[..., 3:4] / (self.S * 2)\n",
    "\n",
    "        rect2_x_max = rect2[..., 0:1] + rect2[..., 2:3] / (self.S * 2)\n",
    "        rect2_x_min = rect2[..., 0:1] - rect2[..., 2:3] / (self.S * 2)\n",
    "        rect2_y_max = rect2[..., 1:2] + rect2[..., 3:4] / (self.S * 2)\n",
    "        rect2_y_min = rect2[..., 1:2] - rect2[..., 3:4] / (self.S * 2)\n",
    "\n",
    "        overlap_x = torch.minimum(\n",
    "            torch.maximum(torch.zeros_like(rect1_x_max, dtype=torch.float32).to(DEVICE), rect1_x_max - rect2_x_min),\n",
    "            torch.maximum(torch.zeros_like(rect1_x_max, dtype=torch.float32).to(DEVICE), rect2_x_max - rect1_x_min)\n",
    "        )\n",
    "        overlap_y = torch.minimum(\n",
    "            torch.maximum(torch.zeros_like(rect1_x_max, dtype=torch.float32).to(DEVICE), rect1_y_max - rect2_y_min),\n",
    "            torch.maximum(torch.zeros_like(rect1_x_max, dtype=torch.float32).to(DEVICE), rect2_y_max - rect1_y_min)\n",
    "        )\n",
    "\n",
    "        result = overlap_x * overlap_y\n",
    "\n",
    "        return result\n",
    "\n",
    "    def yolo_loss(self, target, pred):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            target : torch.tensor with size (BATCH_SIZE, S, S, 5 + self.C)\n",
    "            pred : torch.tensor with size (BATCH_SIZE, S, S, 5*self.B + self.C)\n",
    "        Output\n",
    "            return : YOLO Loss\n",
    "        \"\"\"\n",
    "        # 0. Availability\n",
    "\n",
    "        # 1. Calculate IOUs and responsibility\n",
    "        IOUs = torch.empty((BATCH_SIZE, self.S, self.S, self.B), dtype=torch.float32).to(self.device)\n",
    "        for i in range(self.B):\n",
    "            IOUs[..., i:i+1] = self.IOU(target[..., :4], pred[..., range(5*i, 5*i + 4)])\n",
    "\n",
    "        obj_i = torch.any(target, dim=3, keepdim=True).int()\n",
    "\n",
    "        maximum_iou_values = torch.max(IOUs, dim=3, keepdim=True)[0]\n",
    "        maximum_iou_mask = torch.ge(IOUs, maximum_iou_values).int()\n",
    "        obj_ij = maximum_iou_mask * obj_i\n",
    "\n",
    "        # 2. Calculate Losses\n",
    "        # 2.1. Localization Loss\n",
    "        local_x_loss = torch.sum(obj_ij * F.mse_loss(target[..., 0:1], pred[..., [5*i for i in range(self.B)]], reduce='none'))\n",
    "        local_y_loss = torch.sum(obj_ij * F.mse_loss(target[..., 1:2], pred[..., [5*i+1 for i in range(self.B)]], reduce='none'))\n",
    "        local_w_loss = torch.sum(obj_ij * F.mse_loss(torch.sqrt(target[..., 2:3]), torch.sqrt(pred[..., [5*i+2 for i in range(self.B)]]), reduce='none'))\n",
    "        local_h_loss = torch.sum(obj_ij * F.mse_loss(torch.sqrt(target[..., 3:4]), torch.sqrt(pred[..., [5*i+3 for i in range(self.B)]]), reduce='none'))\n",
    "\n",
    "        local_loss = self.lambda_coord * (local_x_loss + local_y_loss + local_w_loss + local_h_loss)\n",
    "\n",
    "        # 2.2. Confidence Loss\n",
    "        confidence_loss = torch.sum((1 + self.lambda_noobj * (1 - obj_ij)) * F.mse_loss(target[..., 4:5], pred[..., [5*i+4 for i in range(self.B)]], reduction='none'))\n",
    "\n",
    "        # 2.3. Classification Loss\n",
    "        class_loss = torch.sum(obj_i * torch.sum((F.mse_loss(target[..., 5:], pred[..., 5*self.B:], reduction='none')), dim=3, keepdim=True))\n",
    "\n",
    "        # 2.4. Total Loss\n",
    "        loss = local_loss + confidence_loss + class_loss\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [-1, 192, 224, 224]          28,416\n",
      "       BatchNorm2d-2        [-1, 192, 224, 224]             384\n",
      "         MaxPool2d-3        [-1, 192, 112, 112]               0\n",
      "            Conv2d-4        [-1, 256, 112, 112]         442,624\n",
      "       BatchNorm2d-5        [-1, 256, 112, 112]             512\n",
      "         MaxPool2d-6          [-1, 256, 56, 56]               0\n",
      "            Conv2d-7          [-1, 128, 56, 56]          32,896\n",
      "       BatchNorm2d-8          [-1, 128, 56, 56]             256\n",
      "            Conv2d-9          [-1, 256, 56, 56]         295,168\n",
      "      BatchNorm2d-10          [-1, 256, 56, 56]             512\n",
      "           Conv2d-11          [-1, 256, 56, 56]          65,792\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 512, 56, 56]       1,180,160\n",
      "      BatchNorm2d-14          [-1, 512, 56, 56]           1,024\n",
      "        MaxPool2d-15          [-1, 512, 28, 28]               0\n",
      "           Conv2d-16          [-1, 256, 28, 28]         131,328\n",
      "      BatchNorm2d-17          [-1, 256, 28, 28]             512\n",
      "           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n",
      "      BatchNorm2d-19          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-20          [-1, 256, 28, 28]         131,328\n",
      "      BatchNorm2d-21          [-1, 256, 28, 28]             512\n",
      "           Conv2d-22          [-1, 512, 28, 28]       1,180,160\n",
      "      BatchNorm2d-23          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-24          [-1, 256, 28, 28]         131,328\n",
      "      BatchNorm2d-25          [-1, 256, 28, 28]             512\n",
      "           Conv2d-26          [-1, 512, 28, 28]       1,180,160\n",
      "      BatchNorm2d-27          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-28          [-1, 256, 28, 28]         131,328\n",
      "      BatchNorm2d-29          [-1, 256, 28, 28]             512\n",
      "           Conv2d-30          [-1, 512, 28, 28]       1,180,160\n",
      "      BatchNorm2d-31          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-32          [-1, 512, 28, 28]         262,656\n",
      "      BatchNorm2d-33          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-34         [-1, 1024, 28, 28]       4,719,616\n",
      "      BatchNorm2d-35         [-1, 1024, 28, 28]           2,048\n",
      "        MaxPool2d-36         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-37          [-1, 512, 14, 14]         524,800\n",
      "      BatchNorm2d-38          [-1, 512, 14, 14]           1,024\n",
      "           Conv2d-39         [-1, 1024, 14, 14]       4,719,616\n",
      "      BatchNorm2d-40         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-41          [-1, 512, 14, 14]         524,800\n",
      "      BatchNorm2d-42          [-1, 512, 14, 14]           1,024\n",
      "           Conv2d-43         [-1, 1024, 14, 14]       4,719,616\n",
      "      BatchNorm2d-44         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-45         [-1, 1024, 14, 14]       9,438,208\n",
      "      BatchNorm2d-46         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-47           [-1, 1024, 7, 7]       9,438,208\n",
      "      BatchNorm2d-48           [-1, 1024, 7, 7]           2,048\n",
      "           Conv2d-49           [-1, 1024, 7, 7]       9,438,208\n",
      "      BatchNorm2d-50           [-1, 1024, 7, 7]           2,048\n",
      "           Conv2d-51           [-1, 1024, 7, 7]       9,438,208\n",
      "      BatchNorm2d-52           [-1, 1024, 7, 7]           2,048\n",
      "           Linear-53                 [-1, 4096]     205,524,992\n",
      "           Linear-54                 [-1, 1470]       6,022,590\n",
      "================================================================\n",
      "Total params: 272,089,278\n",
      "Trainable params: 272,089,278\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 2.30\n",
      "Forward/backward pass size (MB): 349.93\n",
      "Params size (MB): 1037.94\n",
      "Estimated Total Size (MB): 1390.17\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = YOLO().to(DEVICE)\n",
    "\n",
    "from torchsummary import summary\n",
    "summary(model, (3, 448, 448))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34327/789914508.py:157: UserWarning: Using a target size (torch.Size([4, 7, 7, 2])) that is different to the input size (torch.Size([4, 7, 7, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  local_x_loss = torch.sum(obj_ij * F.mse_loss(target[..., 0:1], pred[..., [5*i for i in range(self.B)]], reduce='none'))\n",
      "/home/kookies371/miniconda3/envs/yolo/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/tmp/ipykernel_34327/789914508.py:158: UserWarning: Using a target size (torch.Size([4, 7, 7, 2])) that is different to the input size (torch.Size([4, 7, 7, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  local_y_loss = torch.sum(obj_ij * F.mse_loss(target[..., 1:2], pred[..., [5*i+1 for i in range(self.B)]], reduce='none'))\n",
      "/tmp/ipykernel_34327/789914508.py:159: UserWarning: Using a target size (torch.Size([4, 7, 7, 2])) that is different to the input size (torch.Size([4, 7, 7, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  local_w_loss = torch.sum(obj_ij * F.mse_loss(torch.sqrt(target[..., 2:3]), torch.sqrt(pred[..., [5*i+2 for i in range(self.B)]]), reduce='none'))\n",
      "/tmp/ipykernel_34327/789914508.py:160: UserWarning: Using a target size (torch.Size([4, 7, 7, 2])) that is different to the input size (torch.Size([4, 7, 7, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  local_h_loss = torch.sum(obj_ij * F.mse_loss(torch.sqrt(target[..., 3:4]), torch.sqrt(pred[..., [5*i+3 for i in range(self.B)]]), reduce='none'))\n",
      "/tmp/ipykernel_34327/789914508.py:165: UserWarning: Using a target size (torch.Size([4, 7, 7, 2])) that is different to the input size (torch.Size([4, 7, 7, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  confidence_loss = torch.sum((1 + self.lambda_noobj * (1 - obj_ij)) * F.mse_loss(target[..., 4:5], pred[..., [5*i+4 for i in range(self.B)]], reduction='none'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(data_loader, 0):\n",
    "        # TERMINATE\n",
    "        if i > 10:\n",
    "            break\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = model.yolo_loss(labels, outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print\n",
    "        print(loss.item())\n",
    "        \n",
    "        # print statistics\n",
    "        # running_loss += loss.item()\n",
    "        # if i % 100 == 99:    # print every 2000 mini-batches\n",
    "        #     print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
    "        #     running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8b1858743e4fd8a5fe86f4b7bd9aae63793f6f08707e6f4b0678979bba351d59"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('yolo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
